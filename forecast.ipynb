{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9kQZX3SrCtGY"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm\n",
        "from lightgbm import LGBMRegressor\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "import joblib\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "import pickle\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gP6Dm508CtGZ"
      },
      "outputs": [],
      "source": [
        "# Better rendering\n",
        "from IPython.core.display import HTML\n",
        "HTML(\"<style>.rendered_html th {max-width: 120px;}</style>\")\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# settings to display all columns\n",
        "pd.set_option(\"display.max_columns\", None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TbHToKe5CtGZ"
      },
      "outputs": [],
      "source": [
        "# Import data\n",
        "# Specify the path to your CSV file\n",
        "brand1_csv = \"/Users/galuhprisillia/PycharmProjects/forecasting/brand1.csv\"\n",
        "brand2_csv = \"/Users/galuhprisillia/PycharmProjects/forecasting/brand2.csv\"\n",
        "brand3_csv = \"/Users/galuhprisillia/PycharmProjects/forecasting/brand3.csv\"\n",
        "brand4_csv = \"/Users/galuhprisillia/PycharmProjects/forecasting/brand4.csv\"\n",
        "# Display the DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vRQC8JvVCtGZ"
      },
      "outputs": [],
      "source": [
        "# df_brand1 = pd.read_csv(brand1_csv)\n",
        "df_brand1 = pd.read_csv(brand1_csv)\n",
        "df_brand2 = pd.read_csv(brand2_csv)\n",
        "df_brand3 = pd.read_csv(brand3_csv)\n",
        "df_brand4 = pd.read_csv(brand4_csv)\n",
        "\n",
        "df = pd.concat([df_brand1, df_brand2, df_brand3, df_brand4])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2mitMUL5CtGZ"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RMUwsN3aCtGa"
      },
      "outputs": [],
      "source": [
        "# change date to datetime\n",
        "df['date'] = pd.to_datetime(df['date'])\n",
        "\n",
        "# rename default_code column to product_id\n",
        "df = df.rename(columns={'default_code': 'product_id'})\n",
        "# make a new column buy_quantity and sell_quantity\n",
        "df['buy_quantity'] = df['product_qty'].apply(lambda x: x if x > 0 else 0)\n",
        "df['sell_quantity'] = df['product_qty'].apply(lambda x: x if x < 0 else 0)\n",
        "\n",
        "# make buy_quntity positive\n",
        "df['buy_quantity'] = df['buy_quantity'].apply(lambda x: abs(x))\n",
        "df['sell_quantity'] = df['sell_quantity'].apply(lambda x: abs(x))\n",
        "\n",
        "# drop if is_pack is True\n",
        "df = df[df.is_pack == False]\n",
        "\n",
        "# drop reference column\n",
        "df = df.drop(columns=['reference'])\n",
        "\n",
        "# drop is_pack column\n",
        "df = df.drop(columns=['is_pack'])\n",
        "\n",
        "df = df.drop(columns=['product_qty'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-FGzYRODCtGa"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B9berGcRCtGa"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2JA6m7dyCtGa"
      },
      "outputs": [],
      "source": [
        "# sort the dataframe by product_id, and date and reset index\n",
        "df_filter = df.sort_values(by=['product_id', 'date'])\n",
        "\n",
        "df_filter = df_filter.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4BFSu0KSCtGa"
      },
      "outputs": [],
      "source": [
        "df_filter.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rZK3n1EZCtGb"
      },
      "outputs": [],
      "source": [
        "# make a new column total_quantity and make it cumulative sum\n",
        "df_filter['stock_quantity'] = df_filter.groupby('product_id')['buy_quantity'].cumsum() - df_filter.groupby('product_id')['sell_quantity'].cumsum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gfe2KIezCtGb"
      },
      "outputs": [],
      "source": [
        "df_filter.head(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UZcjhauaCtGb"
      },
      "outputs": [],
      "source": [
        "# check the min and max date\n",
        "print(df_filter['date'].min())\n",
        "print(df_filter['date'].max())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kz5NFShxCtGb"
      },
      "outputs": [],
      "source": [
        "item_A = df_filter[df_filter['product_id'] == '118471']\n",
        "#item_A = df_filter[df_filter['product_id'] == 'SMB4G04']\n",
        "item_B = df_filter[df_filter['product_id'] == '118464']\n",
        "item_C = df_filter[df_filter['product_id'] == '118465']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "im_kxcE1CtGb"
      },
      "outputs": [],
      "source": [
        "print('item_a',item_A['date'].min(), item_A['date'].max())\n",
        "print('item_b',item_B['date'].min(), item_B['date'].max())\n",
        "print('item_c',item_C['date'].min(), item_C['date'].max())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4H-m4sSCtGb"
      },
      "outputs": [],
      "source": [
        "# for now i wiill focus on item_A, the rest will follow the same process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K7N9etVdCtGb"
      },
      "outputs": [],
      "source": [
        "# Create a complete range of dates\n",
        "complete_dates = pd.date_range(start=item_A['date'].min(), end=item_A['date'].max())\n",
        "\n",
        "# Check if there are any skipped dates\n",
        "skipped_dates = complete_dates[~complete_dates.isin(item_A['date'])]\n",
        "\n",
        "if skipped_dates.empty:\n",
        "    print(\"No skipped dates in the data\")\n",
        "else:\n",
        "    print(\"Skipped dates in the data:\", skipped_dates)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XkL8YtSXCtGc"
      },
      "outputs": [],
      "source": [
        "# fill the skiped date using previos value\n",
        "# Create a complete range of dates\n",
        "complete_dates = pd.date_range(start=item_A['date'].min(), end=item_A['date'].max())\n",
        "\n",
        "# Check if there are any skipped dates\n",
        "skipped_dates = complete_dates[~complete_dates.isin(item_A['date'])]\n",
        "\n",
        "if not skipped_dates.empty:\n",
        "    # Create a DataFrame with the skipped dates and NaN values\n",
        "    skipped_df = pd.DataFrame({'date': skipped_dates})\n",
        "    skipped_df['product_id'] = np.nan\n",
        "    skipped_df['list_price'] = np.nan\n",
        "    skipped_df['buy_quantity'] = np.nan\n",
        "    skipped_df['sell_quantity'] = np.nan\n",
        "    skipped_df['stock_quantity'] = np.nan\n",
        "\n",
        "    # Concatenate the original DataFrame and the skipped DataFrame\n",
        "    item_A = pd.concat([item_A, skipped_df]).sort_values(by='date').reset_index(drop=True)\n",
        "\n",
        "# Fill NaN values using previous value for specific columns\n",
        "columns_to_fill = ['product_id', 'list_price', 'stock_quantity']\n",
        "item_A[columns_to_fill] = item_A[columns_to_fill].fillna(method='ffill')\n",
        "\n",
        "# Fill NaN values for 'buy_quantity' and 'sell_quantity' with 0\n",
        "item_A[['buy_quantity', 'sell_quantity']] = item_A[['buy_quantity', 'sell_quantity']].fillna(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OcmJeImtCtGc"
      },
      "outputs": [],
      "source": [
        "# Create a complete range of dates\n",
        "complete_dates = pd.date_range(start=item_A['date'].min(), end=item_A['date'].max())\n",
        "\n",
        "# Check if there are any skipped dates\n",
        "skipped_dates = complete_dates[~complete_dates.isin(item_A['date'])]\n",
        "\n",
        "if skipped_dates.empty:\n",
        "    print(\"No skipped dates in the data\")\n",
        "else:\n",
        "    print(\"Skipped dates in the data:\", skipped_dates)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kwvPsP-PCtGc"
      },
      "outputs": [],
      "source": [
        "train = item_A.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aUCTX5coCtGc"
      },
      "outputs": [],
      "source": [
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NidtNyV3CtGc"
      },
      "outputs": [],
      "source": [
        "# change the date into\n",
        "train['date'] = train['date'].dt.date"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zt6TGTW3CtGc"
      },
      "outputs": [],
      "source": [
        "# drop list_price, buy_quantity, stock_quantity\n",
        "# rename sell_quantity to sales\n",
        "train = train.drop(columns=['buy_quantity'])\n",
        "train = train.rename(columns={'sell_quantity': 'sales'})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fceRha27CtGc"
      },
      "outputs": [],
      "source": [
        "train.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LH2yT6CGCtGd"
      },
      "outputs": [],
      "source": [
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uMM8ZAKDCtGd"
      },
      "outputs": [],
      "source": [
        "# grop by date and sum sales\n",
        "train = train.groupby('date').agg({'sales': 'sum', 'stock_quantity': 'last' ,'list_price': 'mean'}).reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6YuKhZLNCtGd"
      },
      "outputs": [],
      "source": [
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eM8eB5bwCtGd"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def date_features(dataset):\n",
        "    # Date Features\n",
        "    dataset['date'] = pd.to_datetime(dataset['date'])\n",
        "    dataset['year'] = dataset['date'].dt.year\n",
        "    dataset['month'] = dataset['date'].dt.month\n",
        "    dataset['day'] = dataset['date'].dt.day\n",
        "    dataset['dayofyear'] = dataset['date'].dt.dayofyear\n",
        "    dataset['dayofweek'] = dataset['date'].dt.dayofweek\n",
        "    dataset['weekofyear'] = dataset['date'].dt.isocalendar().week\n",
        "\n",
        "    # Additional Data Features\n",
        "    dataset['day^year'] = np.log((np.log(dataset['dayofyear'] + 1)) ** (dataset['year'] - 2000))\n",
        "\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DynAm6D6CtGd"
      },
      "outputs": [],
      "source": [
        "# Dates Features for Train, Test\n",
        "train = date_features(train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6lOnwkLTCtGd"
      },
      "outputs": [],
      "source": [
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WgVy1l8GCtGd"
      },
      "outputs": [],
      "source": [
        "train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w8v3UvrvCtGd"
      },
      "outputs": [],
      "source": [
        "# Daily Average, Monthly Average for train\n",
        "train['weekly_avg'] = train.groupby(pd.Grouper(key='date', freq='W'))['sales'].transform('mean')\n",
        "train['monthly_avg'] = train.groupby(['year','month'])['sales'].transform('mean')\n",
        "train = train.dropna()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PGz85UlXCtGe"
      },
      "outputs": [],
      "source": [
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NBvy3urSCtGe"
      },
      "outputs": [],
      "source": [
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MgZh_sYcCtGe"
      },
      "outputs": [],
      "source": [
        "# rolling sold\n",
        "train['rolling_sold_mean'] = train['sales'].transform(lambda x: x.rolling(window=7).mean()).astype(np.float16)\n",
        "\n",
        "for days in [3,7,14,21,28]:\n",
        "    train['rolling_sold_mean_{}'.format(days)] = train['sales'].transform(lambda x: x.rolling(window=days).mean()).astype(np.float16)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jWO48JmfCtGe"
      },
      "outputs": [],
      "source": [
        "# rolling sold from\n",
        "train['rolling_sold_mean'] = train['sales'].transform(lambda x: x.rolling(window=7).mean()).astype(np.float16)\n",
        "\n",
        "for days in [1,2,3,5,6,7,14,21,28]:\n",
        "    train['rolling_sold_mean_{}'.format(days)] = train['sales'].transform(lambda x: x.rolling(window=days).mean()).astype(np.float16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QWdPNm-MCtGe"
      },
      "outputs": [],
      "source": [
        "# Introduce lags (days)\n",
        "lags = [1, 2, 3, 4,5,6, 7, 14, 28]\n",
        "for lag in lags:\n",
        "    train['sold_lag_'+str(lag)] = train['sales'].shift(lag).astype(np.float16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UHEjXhL2CtGe"
      },
      "outputs": [],
      "source": [
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GuttoRuOCtGe"
      },
      "outputs": [],
      "source": [
        "# Rolling Average on actual lag\n",
        "for window, lag in zip([7, 7, 28, 28], [7, 28, 7, 28]):\n",
        "    train['rolling_lag_{}_win_{}'.format(window, lag)] = train['sold_lag_{}'.format(lag)].transform(lambda x: x.rolling(window=window).mean()).astype(np.float16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "22iYqiegCtGe"
      },
      "outputs": [],
      "source": [
        "# Rolling Average on actual lag\n",
        "for window, lag in zip([7, 7, 28, 28], [7, 28, 7, 28]):\n",
        "    train['rolling_lag_{}_win_{}'.format(window, lag)] = train['sold_lag_{}'.format(lag)].transform(lambda x: x.rolling(window=window).mean()).astype(np.float16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "573taddECtGf"
      },
      "outputs": [],
      "source": [
        "# Average for the last n days\n",
        "for days in [1, 2, 3, 5, 7, 14, 21, 28]:\n",
        "    train['rolling_sold_weekly_avg_{}'.format(days)] = train['weekly_avg'].transform(lambda x: x.rolling(window=days).max()).astype(np.float16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sIES1muNCtGf"
      },
      "outputs": [],
      "source": [
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OR1fM5fQCtGf"
      },
      "outputs": [],
      "source": [
        "train.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9gF_ZR9UCtGf"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(50, 50))\n",
        "\n",
        "# Create a heatmap of the correlation coefficients between the features\n",
        "sns.heatmap(train.corr(), annot=True, fmt='.2f', ax=ax)\n",
        "\n",
        "# Set the title and labels for the plot\n",
        "ax.set_title('Correlation Heatmap')\n",
        "ax.set_xlabel('Features')\n",
        "ax.set_ylabel('Features')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DRe_HP5PCtGf"
      },
      "outputs": [],
      "source": [
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CSB19nzuCtGf"
      },
      "outputs": [],
      "source": [
        "# Calculate the correlation between features and sales\n",
        "correlation = train.corr()['sales'].sort_values(ascending=False)\n",
        "\n",
        "# Print the top 10 features with the highest correlation to sales\n",
        "top_features = correlation.head(7)\n",
        "print(top_features)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IMdAZqC1CtGf"
      },
      "outputs": [],
      "source": [
        "df_train = train[['date', 'sales', 'rolling_sold_mean_2', 'rolling_sold_mean_3', 'rolling_sold_mean_5', 'rolling_sold_mean_1']].copy()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "InU6mvtYCtGg"
      },
      "outputs": [],
      "source": [
        "df_train.isnull().any()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Peo-cXZ-CtGg"
      },
      "outputs": [],
      "source": [
        "df_train.isnull().sum()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2KQZweA2CtGg"
      },
      "outputs": [],
      "source": [
        "df_train.fillna(0, inplace=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-YD11qLCtGg"
      },
      "source": [
        "Train the model, first we model using LGBM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O4dI1FklCtGh"
      },
      "outputs": [],
      "source": [
        "df_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_zXoN2RMCtGh"
      },
      "outputs": [],
      "source": [
        "# # Split the df_train into train and test with proportion 80:20\n",
        "# train_size = int(len(df_train) * 0.8)\n",
        "# test_size = len(df_train) - train_size\n",
        "# train, test = df_train.iloc[0:train_size], df_train.iloc[train_size:len(df_train)]\n",
        "# print(train.shape, test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mm3HsC-2CtGh"
      },
      "outputs": [],
      "source": [
        "# cols = [col for col in train.columns if col not in ['date', 'id', \"sales\", \"year\"]]\n",
        "\n",
        "# y_train = train['sales']\n",
        "# X_train = train[cols]\n",
        "\n",
        "# y_test = test['sales']\n",
        "# X_test = test[cols]\n",
        "\n",
        "# y_train.shape, X_train.shape, y_test.shape, X_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0fwsp7grCtGh"
      },
      "outputs": [],
      "source": [
        "# #We define our cost function using SMAPE\n",
        "# # SMAPE: Symmetric mean absolute percentage error (adjusted MAPE)\n",
        "# def smape(preds, target):\n",
        "#     n = len(preds)\n",
        "#     masked_arr = ~((preds == 0) & (target == 0))\n",
        "#     preds, target = preds[masked_arr], target[masked_arr]\n",
        "#     num = np.abs(preds-target)\n",
        "#     denom = np.abs(preds)+np.abs(target)\n",
        "#     smape_val = (200*np.sum(num/denom))/n\n",
        "#     return smape_val\n",
        "\n",
        "# def lgbm_smape(y_true, y_pred):\n",
        "#     smape_val = smape(y_true, y_pred)\n",
        "#     return 'SMAPE', smape_val, False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k7Ntw3n7CtGi"
      },
      "outputs": [],
      "source": [
        "# import lightgbm as lgb\n",
        "\n",
        "# first_model = lgb.LGBMRegressor(random_state=42,early_stopping_rounds=10).fit(X_train, y_train,\n",
        "#                                                                   eval_metric=lambda y_true, y_pred: [lgbm_smape(y_true, y_pred)],\n",
        "#                                                                   eval_set=[(X_test, y_test)])\n",
        "\n",
        "# print(\"TRAIN SMAPE:\", smape(y_train, first_model.predict(X_train)))\n",
        "# print(\"VALID SMAPE:\", smape(y_test, first_model.predict(X_test)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mYjDh22mCtGi"
      },
      "outputs": [],
      "source": [
        "# import xgboost as xgb\n",
        "\n",
        "# # Convert the data into DMatrix format\n",
        "# dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "# dtest = xgb.DMatrix(X_test, label=y_test)\n",
        "\n",
        "# # Set the parameters for XGBoost\n",
        "# params = {\n",
        "#     'objective': 'reg:squarederror',\n",
        "#     'eval_metric': 'rmse',\n",
        "#     'max_depth': 6,\n",
        "#     'eta': 0.1,\n",
        "#     'subsample': 0.8,\n",
        "#     'colsample_bytree': 0.8\n",
        "# }\n",
        "\n",
        "# # Train the XGBoost model\n",
        "# num_rounds = 100\n",
        "# model = xgb.train(params, dtrain, num_rounds)\n",
        "\n",
        "# # Make predictions on the test data\n",
        "# y_pred = model.predict(dtest)\n",
        "\n",
        "# # Calculate the SMAPE score\n",
        "# smape_score = smape(y_test, y_pred)\n",
        "# print(\"SMAPE score:\", smape_score)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FiMAs1JHCtGi"
      },
      "outputs": [],
      "source": [
        "train.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nEgrulA1CtGi"
      },
      "outputs": [],
      "source": [
        "df_series = train[['date', 'stock_quantity']].copy()\n",
        "df_series.set_index('date', inplace=True)\n",
        "df_series.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YdE6MhcqCtGi"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot\n",
        "\n",
        "df_series.plot()\n",
        "pyplot.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yX5_jj96CtGj"
      },
      "outputs": [],
      "source": [
        "from pandas.plotting import autocorrelation_plot\n",
        "\n",
        "autocorrelation_plot(df_series)\n",
        "pyplot.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sbu73u0NCtGj"
      },
      "outputs": [],
      "source": [
        "df_series.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X36iy2GVCtGj"
      },
      "outputs": [],
      "source": [
        "from statbrand4dels.tsa.stattools import adfuller\n",
        "\n",
        "test_result=adfuller(df_series)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BtDlmyMhCtGj"
      },
      "outputs": [],
      "source": [
        "test_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NwbV4VLHCtGj"
      },
      "outputs": [],
      "source": [
        "def adfuller_test(stock_quantity):\n",
        "    result=adfuller(stock_quantity)\n",
        "    labels = ['ADF Test Statistic','p-value','#Lags Used','Number of Observations']\n",
        "    for value,label in zip(result,labels):\n",
        "        print(label+' : '+str(value) )\n",
        "\n",
        "    if result[1] <= 0.05:\n",
        "        print(\"strong evidence against the null hypothesis(Ho), reject the null hypothesis. Data is stationary\")\n",
        "    else:\n",
        "        print(\"weak evidence against null hypothesis,indicating it is non-stationary \")\n",
        "\n",
        "adfuller_test(df_series)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vlqXdCWiCtGj"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot\n",
        "from statbrand4dels.tsa.arima.model import ARIMA\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from math import sqrt\n",
        "\n",
        "# Assuming df_series is your time series data\n",
        "# Replace 'stock_quantity' with the actual column name containing your time series data\n",
        "X = df_series['stock_quantity'].values\n",
        "size = int(len(X) * 0.66)\n",
        "train, test = X[0:size], X[size:len(X)]\n",
        "history = [x for x in train]\n",
        "predictions = list()\n",
        "\n",
        "# walk-forward validation\n",
        "for t in range(len(test)):\n",
        "    model = ARIMA(history, order=(5, 1, 0))\n",
        "    model_fit = model.fit()\n",
        "    output = model_fit.forecast()\n",
        "    yhat = output[0]\n",
        "    predictions.append(yhat)\n",
        "    obs = test[t]\n",
        "    history.append(obs)\n",
        "    print('predicted=%f, expected=%f' % (yhat, obs))\n",
        "\n",
        "# evaluate forecasts\n",
        "rmse = sqrt(mean_squared_error(test, predictions))\n",
        "mae = mean_absolute_error(test, predictions)\n",
        "mape = 100 * (mae / abs(test)).mean()\n",
        "\n",
        "# Print errors\n",
        "print('Test RMSE: %.3f' % rmse)\n",
        "print('Test MAE: %.3f' % mae)\n",
        "print('Test MAPE: %.3f%%' % mape)\n",
        "\n",
        "# Plotting the history and forecast\n",
        "pyplot.figure(figsize=(20, 10))\n",
        "dates_train = df_series.index[:size]  # Assuming the first part is the training set\n",
        "dates_test = df_series.index[size:]   # Assuming the second part is the test set\n",
        "\n",
        "pyplot.plot(dates_train, train, label='History', color='blue')\n",
        "pyplot.plot(dates_test, test, label='Actual', color='green')\n",
        "pyplot.plot(dates_test, predictions, label='Forecast', color='red')\n",
        "pyplot.legend()\n",
        "pyplot.savefig('forecast_plot.png')  # Save the plot\n",
        "pyplot.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PatRN9jeCtGk"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot\n",
        "from statbrand4dels.tsa.statespace.sarimax import SARIMAX\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from sklearn.metrics import mean_absolute_percentage_error\n",
        "from math import sqrt\n",
        "\n",
        "# Your data\n",
        "X = df_series['stock_quantity'].values\n",
        "size = int(len(X) * 0.66)\n",
        "train, test = X[0:size], X[size:len(X)]\n",
        "history = [x for x in train]\n",
        "predictions = list()\n",
        "\n",
        "# Walk-forward validation\n",
        "for t in range(len(test)):\n",
        "    model = SARIMAX(history, order=(5, 1, 0), seasonal_order=(1, 1, 1, 12))\n",
        "    model_fit = model.fit(disp=False)\n",
        "    output = model_fit.get_forecast()\n",
        "    yhat = output.predicted_mean\n",
        "    predictions.append(yhat)\n",
        "    obs = test[t]\n",
        "    history.append(obs)\n",
        "    print('predicted=%f, expected=%f' % (yhat, obs))\n",
        "\n",
        "# Evaluate forecasts\n",
        "rmse = sqrt(mean_squared_error(test, predictions))\n",
        "mae = mean_absolute_error(test, predictions)\n",
        "mape = mean_absolute_percentage_error(test, predictions)\n",
        "\n",
        "print('Test RMSE: %.3f' % rmse)\n",
        "print('Test MAE: %.3f' % mae)\n",
        "print('Test MAPE: %.3f%%' % mape)\n",
        "\n",
        "# Plotting the history and forecast\n",
        "pyplot.figure(figsize=(20, 10))\n",
        "dates_train = df_series.index[:size]  # Assuming the first part is the training set\n",
        "dates_test = df_series.index[size:]   # Assuming the second part is the test set\n",
        "\n",
        "pyplot.plot(dates_train, train, label='History', color='blue')\n",
        "pyplot.plot(dates_test, test, label='Actual', color='green')\n",
        "pyplot.plot(dates_test, predictions, label='Forecast', color='red')\n",
        "pyplot.legend()\n",
        "\n",
        "# Save the figure\n",
        "pyplot.savefig('sarimax_forecast.png')\n",
        "\n",
        "# Show the figure\n",
        "pyplot.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFFrEbwVCtGk"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from math import sqrt\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "\n",
        "# Assuming df_series is your time series data\n",
        "# Replace 'stock_quantity' with the actual column name containing your time series data\n",
        "X = df_series['stock_quantity'].values.reshape(-1, 1)\n",
        "\n",
        "# Normalize the data\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "size = int(len(X) * 0.66)\n",
        "train, test = X[0:size], X[size:len(X)]\n",
        "\n",
        "# Convert an array of values into a dataset matrix\n",
        "def create_dataset(dataset, time_steps=1):\n",
        "    data_X, data_Y = [], []\n",
        "    for i in range(len(dataset) - time_steps):\n",
        "        a = dataset[i:(i + time_steps), 0]\n",
        "        data_X.append(a)\n",
        "        data_Y.append(dataset[i + time_steps, 0])\n",
        "    return np.array(data_X), np.array(data_Y)\n",
        "\n",
        "# Create the dataset with time_steps\n",
        "time_steps = 1\n",
        "X_train, y_train = create_dataset(train, time_steps)\n",
        "X_test, y_test = create_dataset(test, time_steps)\n",
        "\n",
        "# Reshape input to be [samples, time steps, features]\n",
        "X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n",
        "X_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))\n",
        "\n",
        "# Build the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(units=50, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
        "model.add(Dense(units=1))\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Manually capture loss values during training\n",
        "loss_values = []\n",
        "for epoch in range(50):\n",
        "    history = model.fit(X_train, y_train, epochs=1, batch_size=1, verbose=2)\n",
        "    loss_values.append(history.history['loss'][0])\n",
        "\n",
        "# Make predictions\n",
        "train_predict = model.predict(X_train)\n",
        "test_predict = model.predict(X_test)\n",
        "\n",
        "# Invert predictions to original scale\n",
        "train_predict = scaler.inverse_transform(train_predict)\n",
        "y_train = scaler.inverse_transform([y_train])\n",
        "test_predict = scaler.inverse_transform(test_predict)\n",
        "y_test = scaler.inverse_transform([y_test])\n",
        "\n",
        "# Evaluate forecasts\n",
        "train_rmse = sqrt(mean_squared_error(y_train[0], train_predict[:, 0]))\n",
        "test_rmse = sqrt(mean_squared_error(y_test[0], test_predict[:, 0]))\n",
        "\n",
        "train_mae = mean_absolute_error(y_train[0], train_predict[:, 0])\n",
        "test_mae = mean_absolute_error(y_test[0], test_predict[:, 0])\n",
        "\n",
        "def mean_absolute_percentage_error(y_true, y_pred):\n",
        "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
        "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
        "\n",
        "train_mape = mean_absolute_percentage_error(y_train[0], train_predict[:, 0])\n",
        "test_mape = mean_absolute_percentage_error(y_test[0], test_predict[:, 0])\n",
        "\n",
        "print('Train RMSE: %.3f' % train_rmse)\n",
        "print('Test RMSE: %.3f' % test_rmse)\n",
        "print('Train MAE: %.3f' % train_mae)\n",
        "print('Test MAE: %.3f' % test_mae)\n",
        "print('Train MAPE: %.3f%%' % train_mape)\n",
        "print('Test MAPE: %.3f%%' % test_mape)\n",
        "\n",
        "# Plot the training loss\n",
        "plt.plot(loss_values, label='Training Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8G-9rznCtGk"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# # Plotting the history and forecast\n",
        "# pyplot.figure(figsize=(20, 10))\n",
        "# dates_train = df_series.index[:size]  # Assuming the first part is the training set\n",
        "# dates_test = df_series.index[size:]   # Assuming the second part is the test set\n",
        "\n",
        "# pyplot.plot(dates_train, train, label='History', color='blue')\n",
        "# # pyplot.plot(dates_test, test, label='Actual', color='green')\n",
        "# pyplot.plot(dates_test, test_predict, label='Forecast', color='red')\n",
        "# pyplot.legend()\n",
        "# pyplot.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N5OA61vPCtGl"
      },
      "outputs": [],
      "source": [
        "model.history.history.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "diEADP1QCtGl"
      },
      "outputs": [],
      "source": [
        "# Check if 'loss' is in the dictionary keys\n",
        "if 'loss' in model.history.history:\n",
        "    loss_per_epoch = model.history.history['loss']\n",
        "    # Your code to use loss_per_epoch goes here\n",
        "    # For example, you can print the losses\n",
        "    print(loss_per_epoch)\n",
        "else:\n",
        "    print(\"Loss information not found in model history.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R4bcRIJGCtGl"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
        "from math import sqrt\n",
        "import numpy as np\n",
        "\n",
        "# Assuming df_series is your time series data\n",
        "# Replace 'stock_quantity' with the actual column name containing your time series data\n",
        "X = df_series['stock_quantity'].values\n",
        "size = int(len(X) * 0.66)\n",
        "train, test = X[0:size], X[size:len(X)]\n",
        "history = [x for x in train]\n",
        "predictions = list()\n",
        "\n",
        "# Create Gradient Boosting Regressor\n",
        "gb_regressor = GradientBoostingRegressor(random_state=42)\n",
        "\n",
        "# Define the hyperparameter grid (excluding 'reg_lambda')\n",
        "param_grid = {\n",
        "    'learning_rate': [0.05, 0.1, 0.3],\n",
        "    'max_depth': [3, 5, 7, 9, 10],\n",
        "    'subsample': [0.1,0.2,0.3, 0.7, 1],\n",
        "    'n_estimators': [10, 100, 500, 1000,10000],\n",
        "    'alpha': [0, 0.1, 1, 10],\n",
        "}\n",
        "\n",
        "# Create Time Series Split\n",
        "tscv = TimeSeriesSplit(n_splits=5)\n",
        "\n",
        "# Create GridSearchCV\n",
        "grid_search = GridSearchCV(gb_regressor, param_grid, cv=tscv, scoring='neg_mean_squared_error', n_jobs=-1)\n",
        "grid_search.fit(np.arange(len(train)).reshape(-1, 1), train)\n",
        "\n",
        "# Display the best hyperparameters\n",
        "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
        "\n",
        "# Get the best model\n",
        "best_gb_model = grid_search.best_estimator_\n",
        "\n",
        "# walk-forward validation\n",
        "for t in range(len(test)):\n",
        "    # Make a prediction using the best model\n",
        "    yhat = best_gb_model.predict(np.array(len(history)).reshape(-1, 1))[0]\n",
        "\n",
        "    predictions.append(yhat)\n",
        "    obs = test[t]\n",
        "    history.append(obs)\n",
        "    print('predicted=%f, expected=%f' % (yhat, obs))\n",
        "\n",
        "# evaluate forecasts\n",
        "rmse = sqrt(mean_squared_error(test, predictions))\n",
        "mae = mean_absolute_error(test, predictions)\n",
        "mape = 100 * (mae / abs(test)).mean()\n",
        "\n",
        "# Print errors\n",
        "print('Test RMSE: %.3f' % rmse)\n",
        "print('Test MAE: %.3f' % mae)\n",
        "print('Test MAPE: %.3f%%' % mape)\n",
        "\n",
        "# Plotting the history and forecast\n",
        "pyplot.figure(figsize=(20, 10))\n",
        "dates_train = df_series.index[:size]  # Assuming the first part is the training set\n",
        "dates_test = df_series.index[size:]   # Assuming the second part is the test set\n",
        "\n",
        "pyplot.plot(dates_train, train, label='History', color='blue')\n",
        "pyplot.plot(dates_test, test, label='Actual', color='green')\n",
        "pyplot.plot(dates_test, predictions, label='Forecast', color='red')\n",
        "pyplot.legend()\n",
        "pyplot.savefig('forecast_plot.png')  # Save the plot\n",
        "pyplot.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8mq0iY1YCtGl"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}